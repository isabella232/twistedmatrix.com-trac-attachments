Index: twisted/web/http.py
===================================================================
--- twisted/web/http.py	(revision 46695)
+++ twisted/web/http.py	(working copy)
@@ -102,6 +102,14 @@
 from twisted.web.iweb import IRequest, IAccessLogFormatter
 from twisted.web.http_headers import Headers
 
+try:
+    from twisted.web.http2 import H2Connection
+    H2_ENABLED = True
+except ImportError:
+    # TODO: Should H2 be optional?
+    H2_ENABLED = False
+    raise
+
 from twisted.web._responses import (
     SWITCHING,
 
@@ -562,6 +570,13 @@
         which this request was received is closed and which is C{True} after
         that.
     @type _disconnected: C{bool}
+
+    @ivar _queuedHeaders: A C{tuple} of items that would normally be passed to
+        L{HTTPChannel.writeHeaders}. Used when the response is queued to store
+        the eventual headers.
+    @type _queuedHeaders: A C{tuple} of HTTP version (C{bytes}), status code
+        (C{int}), reason phrase (C{bytes}), and headers (C{list} of C{tuple}s
+        of C{(bytes, bytes)}.
     """
     producer = None
     finished = 0
@@ -580,6 +595,7 @@
     content = None
     _forceSSL = 0
     _disconnected = False
+    _queuedHeaders = None
 
     def __init__(self, channel, queued):
         """
@@ -598,7 +614,7 @@
         if queued:
             self.transport = StringTransport()
         else:
-            self.transport = self.channel.transport
+            self.transport = self.channel
 
 
     def _cleanup(self):
@@ -637,7 +653,10 @@
 
         # set transport to real one and send any buffer data
         data = self.transport.getvalue()
-        self.transport = self.channel.transport
+        self.transport = self.channel
+        if self._queuedHeaders:
+            self.transport.writeHeaders(*self._queuedHeaders)
+            self._queuedHeaders = None
         if data:
             self.transport.write(data)
 
@@ -727,8 +746,8 @@
 
         # cache the client and server information, we'll need this later to be
         # serialized and sent with the request so CGIs will work remotely
-        self.client = self.channel.transport.getPeer()
-        self.host = self.channel.transport.getHost()
+        self.client = self.channel.getPeer()
+        self.host = self.channel.getHost()
 
         # Argument processing
         args = self.args
@@ -755,7 +774,7 @@
                         self.args.update(cgiArgs)
                 except:
                     # It was a bad request.
-                    _respondToBadRequestAndDisconnect(self.channel.transport)
+                    _respondToBadRequestAndDisconnect(self.channel)
                     return
             self.content.seek(0, 0)
 
@@ -805,14 +824,14 @@
             if streaming:
                 producer.pauseProducing()
         else:
-            self.transport.registerProducer(producer, streaming)
+            self.channel.registerProducer(producer, streaming)
 
     def unregisterProducer(self):
         """
         Unregister the producer.
         """
         if not self.queued:
-            self.transport.unregisterProducer()
+            self.channel.unregisterProducer()
         self.producer = None
 
 
@@ -870,12 +889,16 @@
 
         if not self.startedWriting:
             # write headers
-            self.write('')
+            self.write(b'')
 
         if self.chunked:
             # write last chunk and closing CRLF
             self.transport.write(b"0\r\n\r\n")
 
+
+        if not self.queued:
+            self.channel.endRequest()
+
         # log request
         if hasattr(self.channel, "factory"):
             self.channel.factory.log(self)
@@ -899,19 +922,18 @@
         if not self.startedWriting:
             self.startedWriting = 1
             version = self.clientproto
-            l = []
-            l.append(
-                version + b" " +
-                intToBytes(self.code) + b" " +
-                self.code_message + b"\r\n")
+            code = intToBytes(self.code)
+            reason = self.code_message
 
+            headers = []
+
             # if we don't have a content length, we send data in
             # chunked mode, so that we can support pipelining in
             # persistent connections.
             if ((version == b"HTTP/1.1") and
                 (self.responseHeaders.getRawHeaders(b'content-length') is None) and
                 self.method != b"HEAD" and self.code not in NO_BODY_CODES):
-                l.append(b'Transfer-Encoding: chunked\r\n')
+                headers.append((b'Transfer-Encoding', 'chunked'))
                 self.chunked = 1
 
             if self.lastModified is not None:
@@ -935,15 +957,16 @@
                             category=DeprecationWarning, stacklevel=2)
                         # Backward compatible cast for non-bytes values
                         value = networkString('%s' % (value,))
-                    l.extend([name, b": ", value, b"\r\n"])
+                    headers.append((name, value))
 
             for cookie in self.cookies:
-                l.append(networkString('Set-Cookie: %s\r\n' % (cookie,)))
+                headers.append((b'Set-Cookie', networkString(cookie)))
 
-            l.append(b"\r\n")
+            if self.queued:
+                self._queuedHeaders = (version, code, reason, headers)
+            else:
+                self.channel.writeHeaders(version, code, reason, headers)
 
-            self.transport.writeSequence(l)
-
             # if this is a "HEAD" request, we shouldn't return any data
             if self.method == b"HEAD":
                 self.write = lambda data: None
@@ -1828,7 +1851,86 @@
             request.connectionLost(reason)
 
 
+    def writeHeaders(self, version, code, reason, headers):
+        """
+        Called by C{Request} objects to write a complete set of HTTP headers to
+        a transport.
 
+        @param version: The HTTP version in use.
+        @type version: C{bytes}
+
+        @param code: The HTTP status code to write.
+        @type code: C{bytes}
+
+        @param reason: The HTTP reason phrase to write.
+        @type reason: C{bytes}
+
+        @param headers: The headers to write to the transport.
+        @type headers: L{twisted.web.http_headers.Headers}
+        """
+        response_line = version + b" " + code + b" " + reason + b"\r\n"
+        headerSequence = [response_line]
+        headerSequence.extend(
+            name + b': ' + value + b"\r\n" for name, value in headers
+        )
+        headerSequence.append("\r\n")
+        self.transport.writeSequence(headerSequence)
+
+
+    def registerProducer(self, producer, streaming):
+        """
+        @see L{IConsumer.registerProducer}
+        """
+        return self.transport.registerProducer(producer, streaming)
+
+
+    def unregisterProducer(self):
+        """
+        @see L{IConsumer.unregisterProducer}
+        """
+        return self.transport.unregisterProducer()
+
+
+    def write(self, data):
+        """
+        Called by C{Request} objects to write response data.
+
+        @param data: The data chunk to write to the stream.
+        @type data: C{bytes}
+        """
+        self.transport.write(data)
+
+
+    def writeSequence(self, iovec):
+        self.transport.writeSequence(iovec)
+
+
+    def getPeer(self):
+        return self.transport.getPeer()
+
+
+    def getHost(self):
+        return self.transport.getHost()
+
+    def loseConnection(self):
+        # TODO: Does this need to be smarter?
+        return self.transport.loseConnection()
+
+
+    def endRequest(self):
+        """
+        Called by C{Request} objects to signal completion of a response.
+
+        This is a no-op in HTTP/1.1.
+
+        @param streamID: The ID of the stream to write the headers to. Unused
+            in HTTP/1.
+        @type streamID: C{int}
+        """
+        pass
+
+
+
 def _respondToBadRequestAndDisconnect(transport):
     """
     This is a quick and dirty way of responding to bad requests.
@@ -1950,6 +2052,71 @@
 
 
 
+class GenericHTTPChannel(object):
+    """
+    A proxy object that wraps one of the HTTP protocol objects, and switches
+    between them depending on TLS negotiated protocol.
+    """
+    def __init__(self):
+        object.__setattr__(self, '_negotiatedProtocol', None)
+        object.__setattr__(self, '_obj', HTTPChannel())
+        object.__setattr__(self, '_queued_actions', [])
+
+        self._obj.requestFactory = Request
+
+
+    def dataReceived(self, data):
+        """
+        A override of dataReceived that checks what protocol we're using.
+        """
+        if self._negotiatedProtocol is None:
+            try:
+                negotiatedProtocol = self.transport.negotiatedProtocol
+            except AttributeError:
+                # Plaintext HTTP, always HTTP/1.1
+                negotiatedProtocol = b'http/1.1'
+
+            if negotiatedProtocol is None:
+                negotiatedProtocol = b'http/1.1'
+
+            if H2_ENABLED and negotiatedProtocol == b'h2':
+                transport = self._obj.transport
+                object.__setattr__(self, '_obj', H2Connection())
+                self._apply_queued_actions()
+                self._obj.makeConnection(transport)
+
+            object.__setattr__(
+                self, '_negotiatedProtocol', negotiatedProtocol
+            )
+            object.__setattr__(self, '_queued_actions', None)
+
+        return self._obj.dataReceived(data)
+
+
+    def _apply_queued_actions(self):
+        for action in self._queued_actions:
+            action[0](self._obj, *action[1:])
+
+
+    def __getattr__(self, attr):
+        return getattr(self._obj, attr)
+
+
+    def __setattr__(self, attr, value):
+        if self._negotiatedProtocol is None:
+            self._queued_actions.append((setattr, attr, value))
+
+        return setattr(self._obj, attr, value)
+
+
+    def __delattr__(self, attr):
+        if self._negotiatedProtocol is None:
+            self._queued_actions.append((delattr, attr))
+
+        return delattr(self._obj, attr)
+
+
+
 class HTTPFactory(protocol.ServerFactory):
     """
     Factory for HTTP server.
@@ -1973,7 +2140,7 @@
         timestamps.
     """
 
-    protocol = HTTPChannel
+    protocol = GenericHTTPChannel
 
     logPath = None
 
Index: twisted/web/http2.py
===================================================================
--- twisted/web/http2.py	(revision 0)
+++ twisted/web/http2.py	(working copy)
@@ -0,0 +1,796 @@
+# -*- test-case-name: twisted.web.test.test_http2 -*-
+# Copyright (c) Twisted Matrix Laboratories.
+# See LICENSE for details.
+"""
+HTTP2 Implementation
+
+This is the basic server-side protocol implementation used by the Twisted
+Web server for HTTP2.  This functionality is intended to be combined with the
+HTTP/1.1 and HTTP/1.0 functionality in twisted.web.http to provide complete
+protocol support for HTTP-type protocols.
+
+Some function is currently missing here, including:
+
+- handling flow control, both remote and local
+- handling remote settings changes
+- deciding on suitable local settings values
+"""
+from __future__ import absolute_import
+
+import io
+import random
+
+from collections import deque
+
+from zope.interface import implementer
+
+import priority
+import h2.connection
+import h2.events
+import h2.exceptions
+
+from twisted.internet.defer import Deferred, inlineCallbacks
+from twisted.internet.interfaces import (
+    IProtocol, ITransport, IConsumer, IPushProducer
+)
+from twisted.internet.protocol import Protocol
+from twisted.internet.task import LoopingCall
+from twisted.protocols.tls import _PullToPush
+
+
+_END_STREAM_SENTINEL = object()
+
+
+
+@implementer(IProtocol, IPushProducer)
+class H2Connection(Protocol):
+    """
+    A class representing a single HTTP/2 connection.
+
+    This implementation of IProtocol works hand in hand with H2Stream. This is
+    because we have the requirement to register multiple producers for a single
+    HTTP/2 connection, one for each stream. The standard Twisted interfaces
+    don't really allow for this, so instead there's a custom interface between
+    the two objects that allows them to work hand-in-hand here.
+    """
+    factory = None
+    site = None
+
+
+    def __init__(self):
+        self.conn = h2.connection.H2Connection(client_side=False)
+        self.streams = {}
+
+        self.priority = priority.PriorityTree()
+        self._consumerBlocked = False
+        self._sendingDeferred = None
+        self._outboundStreamQueues = {}
+
+        # Start the data sending function.
+        self._sender = LoopingCall(self._sendPrioritisedData)
+        self._sender.start(interval=0)
+
+
+    # Implementation of IProtocol
+    def connectionMade(self):
+        """
+        Called by the reactor when a connection is received. May also be called
+        by the GenericHTTPChannel during upgrade to HTTP/2.
+        """
+        self.conn.initiate_connection()
+        self.transport.write(self.conn.data_to_send())
+
+
+    def dataReceived(self, data):
+        """
+        Called whenever a chunk of data is received from the transport.
+
+        @param data: The data received from the transport.
+        @type data: C{bytes}
+        """
+        try:
+            events = self.conn.receive_data(data)
+        except h2.exceptions.ProtocolError as e:
+            print e
+            # A remote protocol error terminates the connection.
+            dataToSend = self.conn.data_to_send()
+            if dataToSend:
+                self.transport.write(dataToSend)
+
+            self.transport.loseConnection()
+            return
+
+        for event in events:
+            # TODO: Consider replacing with dictionary-dispatch.
+            if isinstance(event, h2.events.RequestReceived):
+                self._requestReceived(event)
+            elif isinstance(event, h2.events.DataReceived):
+                self._requestDataReceived(event)
+            elif isinstance(event, h2.events.StreamEnded):
+                self._requestEnded(event)
+            elif isinstance(event, h2.events.StreamReset):
+                self._requestAborted(event)
+            elif isinstance(event, h2.events.WindowUpdated):
+                self._handleWindowUpdate(event)
+            elif isinstance(event, h2.events.PriorityUpdated):
+                self._handlePriorityUpdate(event)
+            elif isinstance(event, h2.events.ConnectionTerminated):
+                self.transport.loseConnection()
+
+        dataToSend = self.conn.data_to_send()
+        if dataToSend:
+            self.transport.write(dataToSend)
+
+
+    def connectionLost(self, reason):
+        """
+        Called when the transport connection is lost.
+
+        Informs all outstanding response handlers that the connection has been
+        lost.
+        """
+        try:
+            self._sender.stop()
+        except Exception:
+            pass
+
+        for stream in self.streams.values():
+            stream.connectionLost(reason)
+
+        self.streams = {}
+
+
+    # Implementation of IPushProducer
+    #
+    # Here's how we handle IPushProducer. We have multiple outstanding
+    # H2Streams. Each of these exposes an IConsumer interface to the response
+    # handler that allows it to push data into the H2Stream. The H2Stream then
+    # writes the data into the H2Connection object.
+    #
+    # The H2Connection needs to manage these writes to account for:
+    #
+    # - flow control
+    # - priority
+    #
+    # We manage each of these in different ways.
+    #
+    # For flow control, we simply use the equivalent of the IPushProducer
+    # interface. We simply tell the H2Stream: "Hey, you can't send any data
+    # right now, sorry!". When that stream becomes unblocked, we free it up
+    # again. This allows the H2Stream to propagate this backpressure up the
+    # chain.
+    #
+    # For priority, we need to keep a backlog of data frames that we can send,
+    # and interleave them appropriately. This backlog is most sensibly kept in
+    # the H2Connection object itself. We keep one queue per stream, which is
+    # where the writes go, and then we have a loop that manages popping these
+    # streams off in priority order.
+    #
+    # Logically then, we go as follows:
+    #
+    # 1. Stream calls writeDataToStream(). This causes a DataFrame to be placed
+    #    on the queue for that stream. It also informs the priority
+    #    implementation that this stream is unblocked.
+    # 2. The _sendPrioritisedData() function spins in a tight loop. Each
+    #    iteration it asks the priority implementation which stream should send
+    #    next, and pops a data frame off that stream's queue. If, after sending
+    #    that frame, there is no data left on that stream's queue, the function
+    #    informs the priority implementation that the stream is blocked.
+    #
+    # If all streams are blocked, or if there are no outstanding streams, the
+    # _sendPrioritisedData function waits to be awoken when more data is ready
+    # to send.
+    #
+    # Note that all of this only applies to *data*. Headers and other control
+    # frames deliberately skip this processing as they are not subject to flow
+    # control or priority constraints.
+    def stopProducing(self):
+        """
+        Stop producing data.
+
+        This tells the H2Connection that its consumer has died, so it must stop
+        producing data for good.
+        """
+        self.connectionLost("stopProducing")
+
+
+    def pauseProducing(self):
+        """
+        Pause producing data.
+
+        Tells the H2Connection that it has produced too much data to process
+        for the time being, and to stop until resumeProducing() is called.
+        """
+        self._consumerBlocked = Deferred()
+
+
+    def resumeProducing(self):
+        """
+        Resume producing data.
+
+        This tells the H2Connection to re-add itself to the main loop and
+        produce more data for the consumer.
+        """
+        if self._consumerBlocked is not None:
+            self._consumerBlocked.callback(None)
+            self._consumerBlocked = None
+
+
+    @inlineCallbacks
+    def _sendPrioritisedData(self):
+        """
+        The data sending loop. Must be used within L{LoopingCall}.
+
+        This function sends data on streams according to the rules of HTTP/2
+        priority. It ensures that the data from each stream is interleved
+        according to the priority signalled by the client, making sure that the
+        connection is used with maximal efficiency.
+
+        This function will execute if data is available: if all data is
+        exhausted, the function will place a deferred onto the C{H2Connection}
+        object and wait until it is called to resume executing.
+        """
+        stream = None
+
+        while stream is None:
+            try:
+                stream = next(self.priority)
+            except priority.DeadlockError:
+                # All streams are currently blocked or not progressing. Wait
+                # until a new one becomes available.
+                assert self._sendingDeferred is None
+                self._sendingDeferred = Deferred()
+                yield self._sendingDeferred
+                self._sendingDeferred = None
+                continue
+
+        # Wait behind the transport.
+        if self._consumerBlocked is not None:
+            yield self._consumerBlocked
+
+        # TODO: Handle streams that currently have no data in them.
+        frameData = self._outboundStreamQueues[stream].popleft()
+        maxFrameSize = self.conn.max_outbound_frame_size
+
+        if frameData is _END_STREAM_SENTINEL:
+            self.conn.end_stream(stream)
+
+            # Doing some cleanup here. Only block the stream in priority, don't
+            self._requestDone(stream)
+        else:
+            # Respect the max frame size.
+            if len(frameData) > maxFrameSize:
+                excessData = frameData[maxFrameSize:]
+                frameData = frameData[:maxFrameSize]
+                self._outboundStreamQueues[stream].appendleft(excessData)
+
+            self.conn.send_data(stream, frameData)
+
+            # If there's no data left, this stream is now blocked.
+            if not self._outboundStreamQueues[stream]:
+                self.priority.block(stream)
+
+        self.transport.write(self.conn.data_to_send())
+
+
+    # Internal functions.
+    def _requestReceived(self, event):
+        """
+        Internal handler for when a request has been received.
+
+        @param event: The Hyper-h2 event that encodes information about the
+            received request.
+        @type event: L{h2.events.RequestReceived}
+        """
+        stream = H2Stream(
+            event.stream_id, self, event.headers, self.requestFactory
+        )
+        stream.site = self.site
+        stream.factory = self.factory
+        self.streams[event.stream_id] = stream
+
+        # Add the stream to the priority tree but immediately block it.
+        try:
+            self.priority.insert_stream(event.stream_id)
+        except priority.DuplicateStreamError:
+            # Stream already in the tree. This can happen if we received a
+            # PRIORITY frame before a HEADERS frame. Just move on: we set the
+            # stream up properly in _handlePriorityUpdate.
+            pass
+        else:
+            self.priority.block(event.stream_id)
+
+
+    def _requestDataReceived(self, event):
+        """
+        Internal handler for when a chunk of data is received for a given
+        request.
+
+        @param event: The Hyper-h2 event that encodes information about the
+            received data.
+        @type event: L{h2.events.DataReceived}
+        """
+        stream = self.streams[event.stream_id]
+        stream.receiveDataChunk(event.data, event.flow_controlled_length)
+
+
+    def _requestEnded(self, event):
+        """
+        Internal handler for when a request is complete, and we expect no
+        further data for that request.
+
+        @param event: The Hyper-h2 event that encodes information about the
+            completed stream.
+        @type event: L{h2.events.StreamEnded}
+        """
+        stream = self.streams[event.stream_id]
+        stream.requestComplete()
+
+
+    def _requestAborted(self, event):
+        """
+        Internal handler for when a request is aborted by a remote peer.
+
+        @param event: The Hyper-h2 event that encodes information about the
+            reset stream.
+        @type event: L{h2.events.StreamReset}
+        """
+        stream = self.streams[event.stream_id]
+        stream.connectionLost("Stream reset")
+        self._requestDone(event.stream_id)
+
+
+    def _handlePriorityUpdate(self, event):
+        """
+        Internal handler for when a stream priority is updated.
+
+        @param event: The Hyper-h2 event that encodes information about the
+            stream reprioritization.
+        @type event: L{h2.events.PriorityUpdate}
+        """
+        try:
+            self.priority.reprioritize(
+                stream_id=event.stream_id,
+                depends_on=event.depends_on or None,
+                weight=event.weight,
+                exclusive=event.exclusive,
+            )
+        except priority.MissingStreamError:
+            # A PRIORITY frame arrived before the HEADERS frame that would
+            # trigger us to insert the stream into the tree. That's fine: we
+            # can create the stream here and mark it as blocked.
+            self.priority.insert_stream(
+                stream_id=event.stream_id,
+                depends_on=event.depends_on or None,
+                weight=event.weight,
+                exclusive=event.exclusive,
+            )
+            self.priority.block(event.stream_id)
+
+
+    def writeHeaders(self, version, code, reason, headers, streamID):
+        """
+        Called by C{Request} objects to write a complete set of HTTP headers to
+        a stream.
+
+        @param version: The HTTP version in use. Unused in HTTP/2.
+        @type version: C{bytes}
+
+        @param code: The HTTP status code to write.
+        @type code: C{bytes}
+
+        @param reason: The HTTP reason phrase to write. Unused in HTTP/2.
+        @type reason: C{bytes}
+
+        @param headers: The headers to write to the stream.
+        @type headers: L{twisted.web.http_headers.Headers}
+
+        @param streamID: The ID of the stream to write the headers to.
+        @type streamID: C{int}
+        """
+        headers.insert(0, (b':status', code))
+        self.conn.send_headers(streamID, headers)
+        self.transport.write(self.conn.data_to_send())
+        self._outboundStreamQueues[streamID] = deque()
+
+
+    def writeDataToStream(self, streamID, data):
+        """
+        May be called by L{H2Stream} objects to write response data to a given
+        stream. Writes a single data frame.
+
+        @param streamID: The ID of the stream to write the data to.
+        @type streamID: C{int}
+
+        @param data: The data chunk to write to the stream.
+        @type data: C{bytes}
+        """
+        self._outboundStreamQueues[streamID].append(data)
+        self.priority.unblock(streamID)
+        if self._sendingDeferred is not None:
+            self._sendingDeferred.callback(streamID)
+            self._sendingDeferred = None
+
+
+    def endRequest(self, streamID):
+        """
+        Called by L{H2Stream} objects to signal completion of a response.
+
+        @param streamID: The ID of the stream to write the data to.
+        @type streamID: C{int}
+        """
+        self._outboundStreamQueues[streamID].append(_END_STREAM_SENTINEL)
+        self.priority.unblock(streamID)
+        if self._sendingDeferred is not None:
+            self._sendingDeferred.callback(streamID)
+            self._sendingDeferred = None
+
+
+    def _requestDone(self, streamID):
+        """
+        Called internally by the data sending loop to clean up state that was
+        being used for the stream. Called when the stream is complete.
+
+        @param streamID: The ID of the stream to clean up state for.
+        @type streamID: C{int}
+        """
+        # TODO: consider renaming?
+        del self._outboundStreamQueues[streamID]
+        self.priority.remove_stream(streamID)
+        del self.streams[streamID]
+
+
+    def remainingOutboundWindow(self, streamID):
+        """
+        Called by a C{H2Stream} object to determine how much room is left in
+        the send window for that stream. Allows C{H2Stream} objects to handle
+        flow control and buffering for their producers.
+
+        @param streamID: The ID of the stream whose flow control window we'll
+            check.
+        @type streamID: C{int}
+        """
+        # TODO: This involves a fair bit of looping and computation for
+        # something that is called a lot. Consider caching values somewhere.
+        windowSize = self.conn.local_flow_control_window(streamID)
+        sendQueue = self._outboundStreamQueues[streamID]
+        alreadyConsumed = sum(len(chunk) for chunk in sendQueue)
+
+        return windowSize - alreadyConsumed
+
+
+    def _handleWindowUpdate(self, event):
+        """
+        Manage flow control windows.
+
+        Streams that are blocked on flow control will register themselves with
+        the connection. This will fire deferreds that wake those streams up and
+        allow them to continue processing.
+        """
+        streamID = event.stream_id
+
+        if streamID:
+            # Update applies only to a specific stream. If we don't have the
+            # stream, that's ok: just ignore it.
+            try:
+                self.streams[streamID].windowUpdated()
+            except KeyError:
+                pass
+        else:
+            # Update strictly applies to all streams.
+            for stream in self.streams.values():
+                stream.windowUpdated()
+
+
+    def getPeer(self):
+        """
+        @see L{ITransport.getPeer}
+        """
+        return self.transport.getPeer()
+
+
+    def getHost(self):
+        """
+        @see L{ITransport.getHost}
+        """
+        return self.transport.getHost()
+
+
+    def openStreamWindow(self, streamID, increment):
+        """
+        Open the stream window by a given increment.
+        """
+        # TODO: Consider whether we want some kind of consolidating logic here.
+        self.conn.increment_flow_control_window(increment, stream_id=streamID)
+        self.conn.increment_flow_control_window(increment, stream_id=None)
+        self.transport.write(self.conn.data_to_send())
+
+
+
+@implementer(ITransport, IConsumer, IPushProducer)
+class H2Stream(object):
+    """
+    A class representing a single HTTP/2 stream.
+
+    This class works hand-in-hand with H2Connection. It acts to provide an
+    implementation of ITransport, IConsumer, and IProducer that work for a
+    single HTTP/2 connection, while tightly cleaving to the interface provided
+    by those interfaces. It does this by having a tight coupling to
+    H2Connection, which allows associating many of the functions of ITransport,
+    IConsumer, and IProducer to objects on a stream-specific level.
+    """
+    def __init__(self, streamID, connection, headers, requestFactory):
+        self.streamID = streamID
+        self.producing = False
+        self.command = None
+        self.path = None
+        self.producer = None
+        self._producerProducing = False
+        self._inboundDataBuffer = deque()
+        self._conn = connection
+        self._request = requestFactory(self, queued=False)
+        self._buffer = io.BytesIO()
+
+        self._convertHeaders(headers)
+
+    def _convertHeaders(self, headers):
+        """
+        This method converts the HTTP/2 header set into something that looks
+        like HTTP/1.1. In particular, it strips the 'special' headers and adds
+        a Host: header.
+        """
+        gotLength = False
+
+        for header in headers:
+            if not header[0].startswith(b':'):
+                gotLength = (
+                    _addHeaderToRequest(self._request, header) or gotLength
+                )
+            elif header[0] == u':method':
+                self.command = header[1].encode('utf-8')
+            elif header[0] == u':path':
+                self.path = header[1].encode('utf-8')
+            elif header[0] == u':authority':
+                # This is essentially the Host: header from HTTP/1.1
+                _addHeaderToRequest(self._request, (u'host', header[1]))
+            else:
+                # There are only 4 acceptable special headers for requests.
+                if not header[0] == u':scheme':
+                    raise RuntimeError(
+                        "Unexpected special header %r" % header[0]
+                    )
+
+        if not gotLength:
+            self._request.gotLength(None)
+
+        self._request.parseCookies()
+
+
+    def receiveDataChunk(self, data, flow_controlled_length):
+        """
+        Called when the connection has received a chunk of data from the
+        underlying transport. If the stream has been registered with a
+        consumer, and is currently able to push data, immediately passes it
+        through. Otherwise, buffers the chunk until we can start producing.
+        """
+        if not self.producing:
+            # Buffer data.
+            self._inboundDataBuffer.append((data, flow_controlled_length))
+        else:
+            self._request.handleContentChunk(data)
+            self._conn.openStreamWindow(self.streamID, flow_controlled_length)
+
+
+    def requestComplete(self):
+        """
+        Called by the L{H2Connection} when the all data for a request has been
+        received. Currently, with the legacy Request object, just calls
+        requestReceived.
+        """
+        self._request.requestReceived(self.command, self.path, b'HTTP/2')
+
+
+    def connectionLost(self, reason):
+        """
+        Called by the L{H2Connection} when a connection is lost or a stream is
+        reset.
+        """
+        self._request.connectionLost(reason)
+
+
+    def windowUpdated(self):
+        """
+        Called by the L{H2Connection} when this stream's flow control window
+        has been opened.
+        """
+        # If we're not blocked on flow control, we don't care.
+        if self.producer and self._producerProducing:
+            return
+
+        # We check whether the stream's flow control window is actually above
+        # 0, and then, if it is, send what we can of our buffered data. Then,
+        # if a producer is registered and we still have space in the window, we
+        # unblock it.
+        remainingWindow = self._conn.remainingOutboundWindow(self.streamID)
+        if not remainingWindow > 0:
+            return
+
+        # Empty the buffer and send what we can.
+        # TODO: Try to preserve the buffer here: we do a lot of copies.
+        data = self._buffer.getvalue()
+        dataToSend = data[:remainingWindow]
+        excessData = data[remainingWindow:]
+        self._conn.writeDataToStream(self.streamID, dataToSend)
+
+        # Create a new buffer: we exhausted the last one with getvalue().
+        self._buffer = io.BytesIO(excessData)
+
+        # If the producer is blocked and we still have room, unblock it.
+        if ((remainingWindow - len(dataToSend)) > 0) and (self.producer):
+            # Still space in the window! We can start producing.
+            self.producer._producerProducing = True
+            self.producer.resumeProducing(  )
+
+
+    def writeHeaders(self, version, code, reason, headers):
+        """
+        Called by the consumer to write headers to the stream.
+        """
+        self._conn.writeHeaders(version, code, reason, headers, self.streamID)
+
+
+    def endRequest(self):
+        """
+        Called by the consumer when they've finished writing data.
+        """
+        self._conn.endRequest(self.streamID)
+
+
+    def requestDone(self, request):
+        """
+        Called by a consumer to clean up whatever permanent state is in use.
+        This is a no-op.
+        """
+        pass
+
+
+    # Implementation: ITransport
+    def write(self, data):
+        """
+        Write a single chunk of data into a data frame.
+        """
+        remainingWindow = self._conn.remainingOutboundWindow(self.streamID)
+
+        if remainingWindow - len(data) > 0:
+            # Best-case, write data immediately.
+            self._conn.writeDataToStream(self.streamID, data)
+            return
+
+        if remainingWindow > 0:
+            # We have some room in flow control, but must then buffer.
+            toWrite = data[:remainingWindow]
+            self._conn.writeDataToStream(self.streamID, toWrite)
+            data = data[remainingWindow:]
+
+        # We buffer what's left, then instruct the producer to pause.
+        self._buffer.write(data)
+        if self.producer is not None:
+            self.producer.pauseProducing()
+            self._producerProducing = False
+
+
+    def writeSequence(self, iovec):
+        """
+        Write a sequence of chunks of data into data frames.
+        """
+        for chunk in iovec:
+            self.write(chunk)
+
+
+    def loseConnection(self):
+        """
+        Close the connection after writing all pending data.
+        """
+        # TODO: How to signal early termination?
+        self._conn.endRequest(self.streamID)
+
+
+    def getPeer(self):
+        """
+        Get information about the peer.
+        """
+        self._conn.getPeer()
+
+
+    def getHost(self):
+        """
+        Similar to getPeer, but for this side of the connection.
+        """
+        self._conn.getHost()
+
+
+    # Implementation: IConsumer
+    def registerProducer(self, producer, streaming):
+        """
+        @see L{IConsumer.registerProducer}
+        """
+        if self.producer:
+            raise ValueError(
+                "registering producer %s before previous one (%s) was "
+                "unregistered" % (producer, self.producer))
+
+        if not streaming:
+            producer = _PullToPush(producer, self)
+            producer.startStreaming()
+
+        self.producer = producer
+        self._producerProducing = True
+
+
+    def unregisterProducer(self):
+        """
+        @see L{IConsumer.unregisterProducer}
+        """
+        # When the producer is unregistered, we're done.
+        self.producer = None
+        self._producerProducing = False
+        self.hasStreamingProducer = None
+
+        # TODO: When is this an error case?
+        #self.loseConnection()
+
+
+    # Implementation: IPushProducer
+    def stopProducing(self):
+        """
+        @see L{IProducer.stopProducing}
+        """
+        self.producing = False
+        # TODO: How to signal abnormal termination?
+        self.loseConnection()
+
+
+    def pauseProducing(self):
+        """
+        @see L{IPushProducer.pauseProducing}
+        """
+        self.producing = False
+
+
+    def resumeProducing(self):
+        """
+        @see L{IPushProducer.resumeProducing}
+        """
+        self.producing = True
+        consumedLength = 0
+
+        while self.producing and self._inboundDataBuffer:
+            # Allow for pauseProducing to be called in response to a call to
+            # resumeProducing.
+            chunk, flow_controlled_length = self._inboundDataBuffer.popleft()
+            consumedLength += flow_controlled_length
+            self._request.handleContentChunk(chunk)
+
+        self._conn.openStreamWindow(self.streamID, consumedLength)
+
+
+
+def _addHeaderToRequest(request, header):
+    """
+    Add a header tuple to a request header object.
+    """
+    requestHeaders = request.requestHeaders
+    name, value = header
+    name, value = name.encode('utf-8'), value.encode('utf-8')
+    values = requestHeaders.getRawHeaders(name)
+
+    if values is not None:
+        values.append(value)
+    else:
+        requestHeaders.setRawHeaders(name, [value])
+
+    if name == b'content-length':
+        request.gotLength(int(value))
+        return True
+
+    return False
Index: twisted/web/tap.py
===================================================================
--- twisted/web/tap.py	(revision 46695)
+++ twisted/web/tap.py	(working copy)
@@ -10,7 +10,7 @@
 
 import os
 
-from twisted.web import server, static, script, demo
+from twisted.web import server, static, script, demo, http
 from twisted.internet import interfaces, reactor
 from twisted.python import usage, reflect, threadpool
 from twisted.python.compat import _PY3
@@ -249,10 +249,31 @@
         personal.setServiceParent(s)
     else:
         if config['https']:
-            from twisted.internet.ssl import DefaultOpenSSLContextFactory
-            i = internet.SSLServer(int(config['https']), site,
-                          DefaultOpenSSLContextFactory(config['privkey'],
-                                                       config['certificate']))
+            from twisted.internet.ssl import CertificateOptions
+            from OpenSSL import crypto
+
+            with open(config['privkey'], 'rb') as f:
+                privateKey = crypto.load_privatekey(
+                    crypto.FILETYPE_PEM, f.read()
+                )
+
+            with open(config['certificate'], 'rb') as f:
+                certificate = crypto.load_certificate(
+                    crypto.FILETYPE_PEM, f.read()
+                )
+
+            acceptableProtocols = [b'http/1.1']
+            if http.H2_ENABLED:
+                acceptableProtocols.insert(0, b'h2')
+
+            i = internet.SSLServer(
+                int(config['https']),
+                site,
+                CertificateOptions(privateKey,
+                                   certificate,
+                                   acceptableProtocols=acceptableProtocols),
+                backlog=128
+            )
             i.setServiceParent(s)
         strports.service(config['port'], site).setServiceParent(s)
 
Index: twisted/web/test/requesthelper.py
===================================================================
--- twisted/web/test/requesthelper.py	(revision 46695)
+++ twisted/web/test/requesthelper.py	(working copy)
@@ -20,6 +20,7 @@
 from twisted.internet.address import IPv4Address
 from twisted.internet.interfaces import ISSLTransport
 
+from twisted.web.http import HTTPChannel
 from twisted.web.http_headers import Headers
 from twisted.web.resource import Resource
 from twisted.web.server import NOT_DONE_YET, Session, Site
@@ -52,6 +53,9 @@
         def registerProducer(self, producer, streaming):
             self.producers.append((producer, streaming))
 
+        def unregisterProducer(self):
+            pass
+
         def loseConnection(self):
             self.disconnected = True
 
@@ -70,7 +74,54 @@
         pass
 
 
+    def writeHeaders(self, version, code, reason, headers):
+        response_line = version + b" " + code + b" " + reason + b"\r\n"
+        headerSequence = [response_line]
+        headerSequence.extend(
+            name + b': ' + value + b"\r\n" for name, value in headers
+        )
+        headerSequence.append("\r\n")
+        self.transport.writeSequence(headerSequence)
 
+
+    def getPeer(self):
+        return self.transport.getPeer()
+
+
+    def getHost(self):
+        return self.transport.getHost()
+
+
+    def registerProducer(self, producer, streaming):
+        self.transport.registerProducer(producer, streaming)
+
+
+    def unregisterProducer(self):
+        self.transport.unregisterProducer()
+
+
+    def write(self, data):
+        self.transport.write(data)
+
+
+    def writeSequence(self, iovec):
+        self.transport.writeSequence(iovec)
+
+
+    def loseConnection(self):
+        self.transport.loseConnection()
+
+
+    def endRequest(self):
+        pass
+
+
+    @property
+    def producers(self):
+        return self.transport.producers
+
+
+
 class DummyRequest(object):
     """
     Represents a dummy or fake request.
Index: twisted/web/test/test_http.py
===================================================================
--- twisted/web/test/test_http.py	(revision 46695)
+++ twisted/web/test/test_http.py	(working copy)
@@ -235,6 +235,7 @@
         self.assertEqual(response, expectedResponse)
 
 
+
 class HTTPLoopbackTests(unittest.TestCase):
 
     expectedHeaders = {b'request': b'/foo/bar',
@@ -1556,8 +1557,10 @@
         For an HTTP 1.0 request, L{http.Request.write} sends an HTTP 1.0
         Response-Line and whatever response headers are set.
         """
-        req = http.Request(DummyChannel(), False)
+        channel = DummyChannel()
         trans = StringTransport()
+        channel.transport = trans
+        req = http.Request(channel, False)
 
         req.transport = trans
 
@@ -1578,8 +1581,10 @@
         L{http.Request.write} casts non-bytes header value to bytes
         transparently.
         """
-        req = http.Request(DummyChannel(), False)
+        channel = DummyChannel()
         trans = StringTransport()
+        channel.transport = trans
+        req = http.Request(channel, False)
 
         req.transport = trans
 
@@ -1610,8 +1615,10 @@
         Response-Line, whatever response headers are set, and uses chunked
         encoding for the response body.
         """
-        req = http.Request(DummyChannel(), False)
+        channel = DummyChannel()
         trans = StringTransport()
+        channel.transport = trans
+        req = http.Request(channel, False)
 
         req.transport = trans
 
@@ -1635,8 +1642,10 @@
         L{http.Request.write} sends an HTTP Response-Line, whatever response
         headers are set, and a last-modified header with that time.
         """
-        req = http.Request(DummyChannel(), False)
+        channel = DummyChannel()
         trans = StringTransport()
+        channel.transport = trans
+        req = http.Request(channel, False)
 
         req.transport = trans
 
Index: twisted/web/test/test_proxy.py
===================================================================
--- twisted/web/test/test_proxy.py	(revision 46695)
+++ twisted/web/test/test_proxy.py	(working copy)
@@ -123,7 +123,15 @@
         self.lostReason = reason
 
 
+    def getPeer(self):
+        return self.transport.getPeer()
 
+
+    def getHost(self):
+        return self.transport.getHost()
+
+
+
 class ProxyClientTests(TestCase):
     """
     Tests for L{ProxyClient}.
Index: twisted/web/test/test_web.py
===================================================================
--- twisted/web/test/test_web.py	(revision 46695)
+++ twisted/web/test/test_web.py	(working copy)
@@ -817,7 +817,7 @@
         resource = HeadlessResource()
         req = self._getReq(resource)
         req.requestReceived(b"HEAD", b"/newrender", b"HTTP/1.0")
-        headers, body = req.transport.getvalue().split(b'\r\n\r\n')
+        body = req.transport.getvalue()
         self.assertEqual(req.code, 200)
         self.assertEqual(body, b'')
 
@@ -838,7 +838,7 @@
 
         request.requestReceived(b"GET", b"/newrender", b"HTTP/1.0")
 
-        headers, body = request.transport.getvalue().split(b'\r\n\r\n')
+        body = request.transport.getvalue()
         self.assertEqual(request.code, 500)
         expected = [
             '',
